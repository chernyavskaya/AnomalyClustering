{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b131221c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MnistDataset' from 'utils' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20861/906912204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMnistDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetGPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MnistDataset' from 'utils' (unknown location)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright Â© dawnranger.\n",
    "#\n",
    "# 2018-05-08 10:15 <dawnranger123@gmail.com>\n",
    "#\n",
    "# Distributed under terms of the MIT license.\n",
    "from __future__ import print_function, division\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
    "from sklearn.metrics import adjusted_rand_score as ari_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Linear\n",
    "\n",
    "from utils import MnistDataset, cluster_acc\n",
    "\n",
    "import setGPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e707b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AE(nn.Module):\n",
    "\n",
    "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
    "                 n_input, n_z):\n",
    "        super(AE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.enc_1 = Linear(n_input, n_enc_1)\n",
    "        self.enc_2 = Linear(n_enc_1, n_enc_2)\n",
    "        self.enc_3 = Linear(n_enc_2, n_enc_3)\n",
    "\n",
    "        self.z_layer = Linear(n_enc_3, n_z)\n",
    "\n",
    "        # decoder\n",
    "        self.dec_1 = Linear(n_z, n_dec_1)\n",
    "        self.dec_2 = Linear(n_dec_1, n_dec_2)\n",
    "        self.dec_3 = Linear(n_dec_2, n_dec_3)\n",
    "\n",
    "        self.x_bar_layer = Linear(n_dec_3, n_input)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # encoder\n",
    "        enc_h1 = F.relu(self.enc_1(x))\n",
    "        enc_h2 = F.relu(self.enc_2(enc_h1))\n",
    "        enc_h3 = F.relu(self.enc_3(enc_h2))\n",
    "\n",
    "        z = self.z_layer(enc_h3)\n",
    "\n",
    "        # decoder\n",
    "        dec_h1 = F.relu(self.dec_1(z))\n",
    "        dec_h2 = F.relu(self.dec_2(dec_h1))\n",
    "        dec_h3 = F.relu(self.dec_3(dec_h2))\n",
    "        x_bar = self.x_bar_layer(dec_h3)\n",
    "\n",
    "        return x_bar, z\n",
    "\n",
    "\n",
    "class IDEC(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_enc_1,\n",
    "                 n_enc_2,\n",
    "                 n_enc_3,\n",
    "                 n_dec_1,\n",
    "                 n_dec_2,\n",
    "                 n_dec_3,\n",
    "                 n_input,\n",
    "                 n_z,\n",
    "                 n_clusters,\n",
    "                 alpha=1,\n",
    "                 pretrain_path='data/ae_mnist.pkl'):\n",
    "        super(IDEC, self).__init__()\n",
    "        self.alpha = 1.0\n",
    "        self.pretrain_path = pretrain_path\n",
    "\n",
    "        self.ae = AE(\n",
    "            n_enc_1=n_enc_1,\n",
    "            n_enc_2=n_enc_2,\n",
    "            n_enc_3=n_enc_3,\n",
    "            n_dec_1=n_dec_1,\n",
    "            n_dec_2=n_dec_2,\n",
    "            n_dec_3=n_dec_3,\n",
    "            n_input=n_input,\n",
    "            n_z=n_z)\n",
    "        # cluster layer\n",
    "        self.cluster_layer = Parameter(torch.Tensor(n_clusters, n_z))\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "    def pretrain(self, path=''):\n",
    "        if path == '':\n",
    "            pretrain_ae(self.ae)\n",
    "        # load pretrain weights\n",
    "        self.ae.load_state_dict(torch.load(self.pretrain_path))\n",
    "        print('load pretrained ae from', path)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_bar, z = self.ae(x)\n",
    "        # cluster\n",
    "        q = 1.0 / (1.0 + torch.sum(\n",
    "            torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.alpha)\n",
    "        q = q.pow((self.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "        return x_bar, q\n",
    "\n",
    "\n",
    "def target_distribution(q):\n",
    "    weight = q**2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "\n",
    "def pretrain_ae(model):\n",
    "    '''\n",
    "    pretrain autoencoder\n",
    "    '''\n",
    "    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    print(model)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "    for epoch in range(200):\n",
    "        total_loss = 0.\n",
    "        for batch_idx, (x, _, _) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            x_bar, z = model(x)\n",
    "            loss = F.mse_loss(x_bar, x)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"epoch {} loss={:.4f}\".format(epoch,\n",
    "                                            total_loss / (batch_idx + 1)))\n",
    "        torch.save(model.state_dict(), args.pretrain_path)\n",
    "    print(\"model saved to {}.\".format(args.pretrain_path))\n",
    "\n",
    "\n",
    "def train_idec():\n",
    "\n",
    "    model = IDEC(\n",
    "        n_enc_1=500,\n",
    "        n_enc_2=500,\n",
    "        n_enc_3=1000,\n",
    "        n_dec_1=1000,\n",
    "        n_dec_2=500,\n",
    "        n_dec_3=500,\n",
    "        n_input=args.n_input,\n",
    "        n_z=args.n_z,\n",
    "        n_clusters=args.n_clusters,\n",
    "        alpha=1.0,\n",
    "        pretrain_path=args.pretrain_path).to(device)\n",
    "\n",
    "    #  model.pretrain('data/ae_mnist.pkl')\n",
    "    model.pretrain()\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # cluster parameter initiate\n",
    "    data = dataset.x\n",
    "    y = dataset.y\n",
    "    data = torch.Tensor(data).to(device)\n",
    "    x_bar, hidden = model.ae(data)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
    "    y_pred = kmeans.fit_predict(hidden.data.cpu().numpy())\n",
    "    nmi_k = nmi_score(y_pred, y)\n",
    "    print(\"nmi score={:.4f}\".format(nmi_k))\n",
    "\n",
    "    hidden = None\n",
    "    x_bar = None\n",
    "\n",
    "    y_pred_last = y_pred\n",
    "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "\n",
    "        if epoch % args.update_interval == 0:\n",
    "\n",
    "            _, tmp_q = model(data)\n",
    "\n",
    "            # update target distribution p\n",
    "            tmp_q = tmp_q.data\n",
    "            p = target_distribution(tmp_q)\n",
    "\n",
    "            # evaluate clustering performance\n",
    "            y_pred = tmp_q.cpu().numpy().argmax(1)\n",
    "            delta_label = np.sum(y_pred != y_pred_last).astype(\n",
    "                np.float32) / y_pred.shape[0]\n",
    "            y_pred_last = y_pred\n",
    "\n",
    "            acc = cluster_acc(y, y_pred)\n",
    "            nmi = nmi_score(y, y_pred)\n",
    "            ari = ari_score(y, y_pred)\n",
    "            print('Iter {}'.format(epoch), ':Acc {:.4f}'.format(acc),\n",
    "                  ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
    "\n",
    "            if epoch > 0 and delta_label < args.tol:\n",
    "                print('delta_label {:.4f}'.format(delta_label), '< tol',\n",
    "                      args.tol)\n",
    "                print('Reached tolerance threshold. Stopping training.')\n",
    "                break\n",
    "        for batch_idx, (x, _, idx) in enumerate(train_loader):\n",
    "\n",
    "            x = x.to(device)\n",
    "            idx = idx.to(device)\n",
    "\n",
    "            x_bar, q = model(x)\n",
    "\n",
    "            reconstr_loss = F.mse_loss(x_bar, x)\n",
    "            kl_loss = F.kl_div(q.log(), p[idx])\n",
    "            loss = args.gamma * kl_loss + reconstr_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='train',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--n_clusters', default=7, type=int)\n",
    "    parser.add_argument('--batch_size', default=256, type=int)\n",
    "    parser.add_argument('--n_z', default=10, type=int)\n",
    "    parser.add_argument('--dataset', type=str, default='mnist')\n",
    "    parser.add_argument('--pretrain_path', type=str, default='data/ae_mnist')\n",
    "    parser.add_argument(\n",
    "        '--gamma',\n",
    "        default=0.1,\n",
    "        type=float,\n",
    "        help='coefficient of clustering loss')\n",
    "    parser.add_argument('--update_interval', default=1, type=int)\n",
    "    parser.add_argument('--tol', default=0.001, type=float)\n",
    "    args = parser.parse_args()\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "    print(\"use cuda: {}\".format(args.cuda))\n",
    "    device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "    if args.dataset == 'mnist':\n",
    "        args.pretrain_path = 'data/ae_mnist.pkl'\n",
    "        args.n_clusters = 10\n",
    "        args.n_input = 784\n",
    "        dataset = MnistDataset()\n",
    "    print(args)\n",
    "    train_idec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
